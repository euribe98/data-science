{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification with naive bayes and k-means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "#from collections import Counter\n",
    " \n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    " \n",
    "# Cleaning the text \n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])  # lower-case. stop words removal\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)         # punctuation\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split()) # lemmatization\n",
    "    processed = re.sub(r\"\\d+\",\"\",normalized)\n",
    "    y = processed.split()\n",
    "    return y\n",
    "\n",
    "def getCleanText(doc):\n",
    "    clean_lines = []\n",
    "    for line in doc:\n",
    "        line = line.strip()\n",
    "        cleaned = clean(line)\n",
    "        cleaned = ' '.join(cleaned)\n",
    "        clean_lines.append(cleaned)\n",
    "    \n",
    "    return clean_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification with naive bayes  \n",
    "\n",
    "Naive Bayes is a Supervised Classification technique. We have a training (in this case sklearn 20 newsgroups) dataset with labels containing the correct answers. Using the training set we train a naive bayes model to predict the classifications on new input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "rec.sport.baseball\n",
      "soc.religion.christian\n",
      "rec.sport.baseball\n",
      "rec.sport.baseball\n",
      "rec.sport.baseball\n",
      "soc.religion.christian\n",
      "rec.sport.baseball\n",
      "soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "cat = ['soc.religion.christian', 'rec.sport.baseball']\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  categories=cat, \n",
    "                                  remove=('headers', 'footers', 'quotes'),\n",
    "                                  shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "list(twenty_train.target_names)\n",
    "\n",
    "# top 10 docs\n",
    "for t in twenty_train.target[:10]:\n",
    "    print(twenty_train.target_names[t])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing text\n",
    "\n",
    "Tokenizing and filtering of stopwords are included in CountVectorizer, \n",
    "which builds a dictionary of features and transforms documents to feature vectors.\n",
    "However longer documents will have higher average count values than shorter documents. The tf-idf weight is a statistical measure to evaluate the importantance of a word in a document from a corpus. Importance increases proportionally to the number of times it appears in the document but is offset by the frequency of the word in the corpus. Term frequency (TF) is how common a word is, inverse document frequency (IDF) is how unique or rare a word is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Take me out to the ball park' => rec.sport.baseball\n",
      "'Jesus says love thy neighbor' => soc.religion.christian\n",
      "'Baseball is ninety percent mental. The other half is physical.' => rec.sport.baseball\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# vectorization\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "\n",
    "# training: naive bayes for classification\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, \n",
    "                          twenty_train.target) # labels\n",
    "\n",
    "\n",
    "# predicting\n",
    "docs_new = ['Take me out to the ball park', \n",
    "            'Jesus says love thy neighbor', \n",
    "            'Baseball is ninety percent mental. The other half is physical.']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "#predicted = km.predict(X_new_tfidf)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to have categorized the docs correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Clustering with K-Means\n",
    " K-means is an Unsupervised Learning Algorithm, where there are no labels with right or wrong answers.\n",
    " K-means clustering is able to gradually learn how to cluster the unlabelled points into groups by analysis of\n",
    " the  mean distance of said points.\n",
    " \n",
    " In this case, I have created a corpus containing quotes about dogs and google without any target or label.  \n",
    " This is used to train the K-means model for prediction of new documnets.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " dog\n",
      " love\n",
      " choose\n",
      " time\n",
      " wasted\n",
      " spent\n",
      " best\n",
      " ive\n",
      " taken\n",
      " photo\n",
      "Cluster 1:\n",
      " google\n",
      " map\n",
      " regularly\n",
      " use\n",
      " feedback\n",
      " impressed\n",
      " incredible\n",
      " app\n",
      " translate\n",
      " tab\n",
      "\n",
      "Top prediction:\n",
      "[0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "docs = [\"Time spent with dogs is never wasted\",\n",
    "\"Dogs choose us, we don't own them.\",\n",
    "\"It is impossible to keep a straight face in the presence of one or more puppies.\",\n",
    "\"A dog is the only thing on earth that loves you more than you love yourself.\",\n",
    "\"No matter how you're feeling, a little dog gonna love you.\"\n",
    "\"There's a saying, when the dog looks at you, the dog is not thinking what kind of a person you are\",\n",
    "\"If you can spell 'Nietzsche' without Google, you deserve a cookie.\",\n",
    "\"Google Translate app is incredible.\",\n",
    "\"If you open 100 tabs in google you get a smiley face.\",\n",
    "\"Best dog photo I've ever taken.\",\n",
    "\"Impressed with google map feedback.\",\n",
    "\"Key promoter extension for Google Chrome.\",\n",
    "\"I use google maps regularly\"]\n",
    "\n",
    "clean_docs = getCleanText(docs)\n",
    "\n",
    "# vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(clean_docs)\n",
    "\n",
    "# train k-means model\n",
    "true_k = 2\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    \n",
    "    \n",
    "print(\"\\nTop prediction:\")\n",
    "new_docs = [\"Dogs are man's best friend\", \n",
    "            \"Cat and dog videos on you-tube are the best\",\n",
    "            \"Google chrome is the worst\",\n",
    "            \"Google is taking over the world\"]\n",
    "\n",
    "# prediction\n",
    "pred = model.predict(vectorizer.transform(new_docs))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to have categorized the docs correctly. The first 2 docs were categorized into cluster 1, while \n",
    "the last 2 were categorized into cluster 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:<br>\n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html <br>\n",
    "https://hackernoon.com/finding-the-most-important-sentences-using-nlp-tf-idf-3065028897a3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
